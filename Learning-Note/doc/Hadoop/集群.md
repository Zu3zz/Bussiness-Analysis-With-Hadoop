# Hadoop 集群规划

- HDFS: NN DN
- YARN: RM NM

- hadoop000 192.168.199.234
  - NN RM
  - DN NM
- hadoop001 192.168.199.235
  - DN NM
- hadoop002 192.168.199.236
  - DN NM

## 详细步骤

- 对每台机器

  - 修改 host 配置

    - 在/etc/hostname 下修改 hostname(hadoop000/hadoop001/hadoop002)
    - 在/etc/hosts 下修改 ip 和 hostname 的映射关系

      1. 192.168.199.234 hadoop000
      2. 192.168.199.235 hadoop001
      3. 192.168.199.236 hadoop002
      4. 192.168.199.23x localhost(试机器而定)

  - 前置安装 ssh 进行免密码登录操作

  ```shell
  ssh-keygen -t rsa
  ```

  在每台 hadoop 机器上进行操作

  ```shell
  ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop000
  ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop001
  ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop002
  ```

  - 安装 java jkd

    - 首先在每台 hadoop 集群机器上部署 jdk
    - 将 jkd bin 配置到系统环境变量(~/.bash_profile)
    - 将 jdk 以及环境变量配置拷贝到其他节点上去(start with hadoop000)

    ```shell
    scp -r jdk1.8.0_91 hadoop@hadoop001:~/app/
    scp -r jdk1.8.0_91 hadoop@hadoop002:~/app/

    scp ~/.bash_profile hadoop@hadoop001:~/
    scp ~/.bash_profile hadoop@hadoop002:~/
    ```

  - Hadoop 部署

    - hadoop-env.sh 配置: JAVA_HOME
    - hdfs-site.xml 配置:

    ```xml
    <property>
      <name>dfs.namenode.name.dir</name>
      <value>/home/hadoop/app/tmp/dfs/name</value>
    </property>
    <property>
      <name>dfs.datanode.data.dir</name>
      <value>/home/hadoop/app/tmp/dfs/data</value>
    </property>
    ```

    - yarn-site.xml

    ```xml
    <!--只在hadoop000机器中进行配置-->
    <property>
      <name>yarn.nodemanager.aux-services</name>
      <value>mapreduce_shuffle</value>
    </property>

    <property>
        <name>yarn.resourcemanager.hostname</name>
        <value>hadoop000</value>
    </property>
    ```

    - mapred-site.xml: 这个文件只有模板 需要自己创建

    ```xml
    <property>
      <name>mapreduce.framework.name</name>
      <value>yarn</value>
    </property>
    ```

    - 配置 slaves
    - 分发 hadoop 到其他机器

    ```shell
    scp -r hadoop-2.6.0-cdh5.15.1 hadoop@hadoop001:~/app/
    scp -r hadoop-2.6.0-cdh5.15.1 hadoop@hadoop002:~/app/

    scp ~/.bash_profile hadoop@hadoop001:~/
    scp ~/.bash_profile hadoop@hadoop002:~/
    ```

    - NN 格式化

    ```shell
    hadoop namenode -format
    ```

    - 在每个机器上启动 HDFS
    - 在每个机器上启动 YRAN
